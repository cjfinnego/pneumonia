# -*- coding: utf-8 -*-
"""Copy of pneumonia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O5_2z7vXHVHkqBm2kOes4K7ENsH9cTpP
"""

import logging
import os

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.metrics import Recall, Accuracy, Precision
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix

# from google.colab import drive

# drive.mount('/content/drive')

# Set the path to the dataset
# dataset_path = "/content/drive/MyDrive/chest_xray"

# Set the parameters for the CNN
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("pneumonia")

# Load the dataset


def prepare_data(dataset_path="data"):
    batch_size = 32
    try:
        train_dir = os.path.join(dataset_path, "train")
        train_normal_dir = os.path.join(train_dir, "NORMAL")
        train_pneumonia_dir = os.path.join(train_dir, "PNEUMONIA")
        train_normal_files = [os.path.join(
            train_normal_dir, filename) for filename in os.listdir(train_normal_dir)]
        train_pneumonia_files = [os.path.join(
            train_pneumonia_dir, filename) for filename in os.listdir(train_pneumonia_dir)]
        train_labels = np.concatenate(
            [np.zeros(len(train_normal_files)), np.ones(len(train_pneumonia_files))])
        train_files, val_files, train_labels, val_labels = train_test_split(
            train_normal_files + train_pneumonia_files,
            train_labels,
            test_size=0.2,
            random_state=42,
            stratify=train_labels
        )
        train_datagen = ImageDataGenerator(
            rescale=1.0/255.0,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True
        )
        train_generator = train_datagen.flow_from_directory(
            train_dir,
            target_size=(150, 150),
            batch_size=batch_size,
            class_mode='binary'
        )
    except Exception as e:
        logger.warning("Error preparing training dataset: " + str(e))
    try:
        val_dir = os.path.join(dataset_path, "val")
        val_normal_dir = os.path.join(val_dir, "NORMAL")
        val_pneumonia_dir = os.path.join(val_dir, "PNEUMONIA")
        val_normal_files = [os.path.join(train_normal_dir, filename)
                            for filename in os.listdir(val_normal_dir)]
        val_pneumonia_files = [os.path.join(
            train_pneumonia_dir, filename) for filename in os.listdir(val_pneumonia_dir)]
        val_labels = np.concatenate(
            [np.zeros(len(val_normal_files)), np.ones(len(val_pneumonia_files))])
        val_test_datagen = ImageDataGenerator(rescale=1.0/255.0)
        val_generator = val_test_datagen.flow_from_directory(
            val_dir,
            target_size=(150, 150),
            batch_size=batch_size,
            class_mode='binary'
        )
    except Exception as e:
        logger.warning("Error preparing validation dataset: " + str(e))
    test_dir = os.path.join(dataset_path, "test")

    test_normal_dir = os.path.join(test_dir, "NORMAL")
    test_pneumonia_dir = os.path.join(test_dir, "PNEUMONIA")

    # Get the file paths for the training and testing data

    test_normal_files = [os.path.join(
        test_normal_dir, filename) for filename in os.listdir(test_normal_dir)]
    test_pneumonia_files = [os.path.join(
        test_pneumonia_dir, filename) for filename in os.listdir(test_pneumonia_dir)]

    # Create the labels for the data

    test_labels = np.concatenate(
        [np.zeros(len(test_normal_files)), np.ones(len(test_pneumonia_files))])

    # Split the data into training and validation sets

    # Data augmentation for training set

    # Data augmentation for validation and testing set

    # Load and preprocess the images
    test_datagen = ImageDataGenerator(rescale=1.0/255.0)
    test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary'
    )
    return test_generator

# Create the CNN model


def create_model():
    input_shape = (150, 150, 3)
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy',
                  metrics=['accuracy', Recall(), Precision()])
    return model

# Train the model


def train_model(model):
    # TODO get batchsize and dataset generators in here
    epochs = 2
    history = model.fit(
        train_generator,
        steps_per_epoch=len(train_files) // batch_size,
        epochs=epochs,
        validation_data=val_generator,
        validation_steps=len(val_files) // batch_size
    )

    model.save('/content/drive/MyDrive/model_e2.h5')

    # Evaluate the model on the test set


def evaluate_model(model):
    """recall = 'TP/(TP + FN)'

    precision = 'TP/(TP + FP)'

    f1score = '2* recall * precision/(precision + recall)'

    can also do cross validation
    """
    test_generator = prepare_data()
    metrics = model.evaluate(test_generator, return_dict=True)
    logger.info("Metrics", metrics)
    precision = metrics['precision']
    recall = metrics['recall']
    f1 = 2 * precision * recall/(precision + recall)
    test_generator.reset()
    predictions = model.predict(test_generator)
    y_true = test_generator.classes  # true predictions
    logits = np.where(predictions > 0.5, 1, 0)
    logger.info(confusion_matrix(y_true, logits))
# f1

# !du -sh /content/drive/MyDrive/model_e5.h5


# model_2 = load_model('/content/drive/MyDrive/model_e2.h5')

# predict = model_2.predict(test_generator)

def main():
    model = create_model()
    evaluate_model(model)


if __name__ == "__main__":
    main()
